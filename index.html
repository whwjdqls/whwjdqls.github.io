<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">

    <title>Jungbin Cho</title>

    <meta name="author" content="Jungbin Cho">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Favicon / external stylesheet (keep if you have them) -->
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <!-- One small rule to keep thumbnails from being squished -->
    <style>
      .paper-img {
        max-width: 160px;   /* fits two-column layout */
        height: auto;       /* preserve aspect ratio */
        object-fit: contain;
      }
    </style>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
      <tbody>
        <tr>
          <td>

            <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Header / Profile ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
              <tbody>
                <tr>
                  <!-- Text column -->
                  <td style="padding:2.5%;width:63%;vertical-align:middle;">
                    <p class="name" style="text-align:center;">Jungbin Cho</p>

                    <p>
                      I am a Master‚Äôs student at
                      <a href="https://www.yonsei.ac.kr/en_sc/">Yonsei University</a>,
                      advised by
                      <a href="https://yj-yu.github.io/home/">Professor Yu</a>. <br>
                      
                      I am currently a visiting researcher with Prof. <a href="https://www.laszlojeni.com/">Laszlo A. Jeni</a> at <a href="https://laszlojeni.com/index.html#lab">CUBE lab</a>, CMU, working on human related projects. <br>
                      
                      <!-- I received my B.S. in
                      <a href="https://yonsei.elsevierpure.com/en/organisations/department-of-electrical-and-electronic-engineering">
                        Electrical and Electronic Engineering
                      </a>
                      from Yonsei University. -->
                    </p>
<!-- 
                    <p style="text-align:center;">
                      <a href="mailto:whwjdqls99@yonsei.ac.kr">Email</a> &nbsp;/&nbsp; -->
                      <!-- <a href="https://www.overleaf.com/read/nrgymmmvhchq#110461">CV</a> &nbsp;/&nbsp;-->
                      <!-- <a href="images/cv.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=DWd9_qQAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://github.com/whwjdqls">Github</a> &nbsp;/&nbsp;
                      <a href="www.linkedin.com/in/jungbin-cho-55302b266">LinkedIn</a>
                    </p> -->

<p style="text-align:center;">
  <a href="mailto:whwjdqls99@yonsei.ac.kr">Email</a> &nbsp;/&nbsp;
  <a href="images/cv.pdf">CV</a> &nbsp;/&nbsp;
  <a href="https://scholar.google.com/citations?user=DWd9_qQAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
  <a href="https://github.com/whwjdqls">Github</a> &nbsp;/&nbsp;
  <a href="https://www.linkedin.com/in/jungbin-cho-55302b266">LinkedIn</a>
</p>
                  </td>

                  <!-- Photo column -->
                  <td style="padding:2.5%;width:37%;max-width:37%;vertical-align:middle;">
                    <a href="images/Jungbin_cho_3.jpg">
                      <img src="images/Jungbin_cho_3.jpg"
                           alt="profile photo"
                           style="width:100%;height:auto;object-fit:contain;">
                    </a>
                  </td>
                </tr>
              </tbody>
            </table> <!-- /header table -->

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ News ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
  <tbody>
    <tr>
      <td style="padding:16px;">
        <h2>News</h2>
        <ul style="list-style-type:none;margin:0;padding-left:0;">
          <!-- 2025 -->
          <li style="margin-bottom:6px;">üìù <b>Nov&nbsp;2025</b> ‚Äî Our paper on hand object interaction reconstruction is out.</li>
          <li style="margin-bottom:6px;">üìù <b>Oct&nbsp;2025</b> ‚Äî New paper on Scene-aware adaptation is out.</li>
          <li style="margin-bottom:6px;">üìç <b>Aug&nbsp;2025</b> ‚Äî I will be joining CMU as a visiting researcher.</li>
          <li style="margin-bottom:6px;">üìù <b>Jun&nbsp;2025</b> ‚Äî Our paper on motion token decoding is accepted to ICCV&nbsp;2025.</li>
          <!-- 2024 -->
          <li style="margin-bottom:6px;">üìù <b>Dec&nbsp;2024</b> ‚Äî Our paper on 3D human face animation accepted to AAAI&nbsp;2025.</li>
          <li style="margin-bottom:6px;">üéì <b>Sep&nbsp;2024</b> ‚Äî I will be starting my master‚Äôs degree at Yonsei!</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Research interests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;">
                    <h2>Research</h2>
                    <p>
My long-term goal is to build a human foundation model: an embodied system that perceives, moves, and interacts with its surroundings and with people, whether as a digital human or a humanoid robot.  
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Publications ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
              <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
              <tbody>
                <!-- AlignHOI-->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/pri4r.png" class="paper-img" alt="sceneadapt thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://whwjdqls.github.io/">
                      <span class="papertitle">Pri4R: Learning World Dynamics for Vision-Language-Action Models with Privileged 4D Representation</span>
                    </a><br>
                  Jisoo Kim*, <strong>Jungbin Cho*</strong>, Sanghyeok Chu, Ananya Bal, Jinhyung Kim, Gunhee Lee, Sihaeng Lee, Seung Hwan Kim, Bohyung Han, Hyunmin Lee, <a href="https://www.laszlojeni.com/">Laszlo A. Jeni</a>, Seungryong Kim<br>
                  
              
                    <em>preprint</em>
                    <p>Endowing world dynamics awareness to Vision Language Action models using 3D point tracking.</p>
                  </td>
                </tr>

                
              <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
              <tbody>
                <!-- AlignHOI-->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/AlignHOI.png" class="paper-img" alt="sceneadapt thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://litingww.github.io/images/research/AlignHOI.pdf">
                      <span class="papertitle">AlignHOI: Hand‚ÄìObject Reconstruction via Alignment and Refinement</span>
                    </a><br>
                  Liting Wen, Xin Lv, <strong>Jungbin Cho</strong>, <a href="https://www.laszlojeni.com/">Laszlo A. Jeni</a>, Xiao-Xiao Long<br>
                  
              
                    <em>preprint</em>
                    <p>Reconstructing hand object interactions from monocular videos.</p>
                  </td>
                </tr>

            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
              <tbody>
                <!-- SceneAdapt -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/scene-adapt.png" class="paper-img" alt="sceneadapt thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://arxiv.org/abs/2510.13044">
                      <span class="papertitle">SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion</span>
                    </a><br>
                    <strong>Jungbin Cho</strong><sup>*</sup>, Minsu Kim<sup>*</sup>,
                    Jisoo Kim,
                  <a href="https://zczcwh.github.io/">Ce Zheng</a>,
                  <a href="https://www.laszlojeni.com/">Laszlo A. Jeni</a>,
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
              <a href="https://yj-yu.github.io/home/">Youngjae Yu</a>, 
              <a href="https://sites.google.com/site/seonjookim/">Seonjoo Kim</a><br>
              
                    <em>preprint</em>
                    <p>Adapting text-to-motion models to become scene-aware using only scene-motion paired data.</p>
                  </td>
                </tr>

                <!-- DisCoRD -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/Discord.png" class="paper-img" alt="DisCoRD thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://arxiv.org/abs/2411.19527">
                      <span class="papertitle">DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding</span>
                    </a><br>
                    <strong>Jungbin Cho</strong><sup>*</sup>, Junwan Kim<sup>*</sup>,
                    Jisoo Kim, Minseo Kim, Mingu Kang,
                    <a href="https://www.csehong.com/">Sungeun Hong</a>,
                    <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a>,
                    <a href="https://yj-yu.github.io/home/">Youngjae Yu</a><br>
                    <em>ICCV</em> 2025 (‚ú®<strong><em>Highlight</em></strong> 263/11239=2.34%)
                    <!-- <em>ICCV</em> 2025 (‚ú® <span class="highlight-background">Highlight</span>) -->
                    <p>Generating smooth and natural motion by decoding discrete motion tokens with rectified flow.</p>
                  </td>
                </tr>

                <!-- DEEPTalk -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/DEEPTalk.png" class="paper-img" alt="DEEPTalk thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://arxiv.org/abs/2408.06010">
                      <span class="papertitle">
                        DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D Face Animation
                      </span>
                    </a><br>
                    Jisoo Kim<sup>*</sup>, <strong>Jungbin Cho</strong><sup>*</sup>, Joonho Park,
                    Soonmin Hwang, Da Eun Kim, Geon Kim,
                    <a href="https://yj-yu.github.io/home/">Youngjae Yu</a><br>
                    <em>AAAI</em> 2025
                    <p>Generating dynamic emotional talking faces using probabilistic embeddings and a temporally hierarchical motion tokenizer.</p>
                  </td>
                </tr>

                <!-- EgoSpeak -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/EgoSpeak.png" class="paper-img" alt="EgoSpeak thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://arxiv.org/abs/2408.06010">
                      <span class="papertitle">
                        EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild
                      </span>
                    </a><br>
                    Junhyeok Kim, Minsoo Kim, Jiwan Chung, <strong>Jungbin Cho</strong>,
                    Jisoo Kim, Sungwoong Kim, Gyeongbo Sim,
                    <a href="https://yj-yu.github.io/home/">Youngjae Yu</a><br>
                    <em>NAACL Findings</em> 2025
                    <p>Detecting when to speak.</p>
                  </td>
                </tr>

                <!-- AVIN-Chat -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/AVIN-Chat.png" class="paper-img" alt="AVIN-Chat thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://arxiv.org/abs/2409.00012">
                      <span class="papertitle">
                        AVIN-Chat: An Audio-Visual Interactive Chatbot System with Emotional State Tuning
                      </span>
                    </a><br>
                    Chanhyuk Park<sup>*</sup>, <strong>Jungbin Cho</strong><sup>*</sup>,
                    Junwan Kim<sup>*</sup>, Seongmin Lee, Jungsu Kim, Sanghoon Lee<br>
                    <em>IJCAI Demo</em> 2024
                    <p>A chatbot system designed for face-to-face interactions, featuring customizable virtual avatars for personalized conversations.</p>
                  </td>
                </tr>

                <!-- VSCHH -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle;">
                    <img src="images/VSCHH.png" class="paper-img" alt="VSCHH thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle;">
                    <a href="https://openaccess.thecvf.com/content/ICCV2023W/RHWC/papers/Jang_VSCHH_2023_A_Benchmark_for_the_View_Synthesis_Challenge_of_ICCVW_2023_paper.pdf">
                      <span class="papertitle">
                        VSCHH 2023: A Benchmark for the View Synthesis Challenge of Human Heads
                      </span>
                    </a><br>
                    Youngkyoon Jang, ..., Hyeseong Kim, <strong>Jungbin Cho</strong>, Dosik Hwang,
                    ..., Stefanos Zafeiriou<br>
                    <em>ICCV Workshop</em> 2023
                    <p>Reconstructing high-resolution 3D human heads from sparse input views.</p>
                  </td>
                </tr>

              </tbody>
            </table>

            <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
            <table style="width:100%;border:0;border-spacing:0;margin:0 auto;">
              <tbody>
                <tr>
                  <td>
                    <br>
                    <p style="text-align:right;font-size:small;">
                      <a href="https://jonbarron.info/">Cool website template by Jon Barron</a>
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            <!-- üåç Visitor Globe Tracker -->
<!--           <div style="width: 300px; margin: 50px auto 0 auto;">
            <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=7aICIrhe3ianncSpcGUuB-N19ad0-2iijPCJxVxdulM"></script>
          </div> -->

             <div style="width: 300px; margin: 50px auto 0 auto;">
            <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=i6ZNv3vJc-2hGXjAGkNz5Zen_NpKwT7kpk1t2ZhwWZo"></script>
            </div> 
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
